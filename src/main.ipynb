{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5aba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torchvision\n",
    "from typing import Union, List, Dict, Any, cast\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import Caltech256, CIFAR10, STL10, Caltech101\n",
    "from torchvision.models import VGG\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a07096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group norm\n",
    "def make_gn_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                if norm_layer is None:\n",
    "                    raise Error(\"Please specify a norm layer\")\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                layers += [conv2d, norm_layer(v//2, v)(), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_gn_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_gn_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11_gn(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_gn_vgg('vgg11_bn', 'A', True, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2402433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel batch\n",
    "def make_bc_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = WConv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                # hyperparameters\n",
    "                num_groups = v//2\n",
    "                eps=1e-08\n",
    "                layers += [conv2d, BCNorm(v, num_groups, eps), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_bc_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_bc_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11_bcwn(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_bc_vgg('vgg11_bn', 'A', True, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b737ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel batch\n",
    "def make_bn_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                # hyperparameters\n",
    "                eps=1e-08\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_bn_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_bn_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11_bn(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_bn_vgg('vgg11_bn', 'A', True, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d962cedf",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ebc3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel batch\n",
    "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                # hyperparameters\n",
    "                raise NotImplemented\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_vgg('vgg11', 'A', False, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1f7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_group_norm_layer(in_channel, out_channel):\n",
    "    def fun():\n",
    "        return nn.GroupNorm(in_channel, out_channel)\n",
    "    return fun\n",
    "\n",
    "# channel norm + weight\n",
    "# weight\n",
    "class WConv2d(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(WConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                 padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.weight\n",
    "        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n",
    "                                  keepdim=True).mean(dim=3, keepdim=True)\n",
    "        weight = weight - weight_mean\n",
    "        std = torch.pow(weight.view(weight.size(0), -1).var(dim=1) + 1e-5, 0.5).view(-1, 1, 1, 1)\n",
    "        weight = weight / std.expand_as(weight)\n",
    "        return F.conv2d(x, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "\n",
    "class BCNorm(nn.Module):\n",
    "#     eps = epsilon\n",
    "    def __init__(self, num_channels, num_groups, eps, estimate=False):\n",
    "        super(BCNorm, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_groups = num_groups\n",
    "        self.eps = eps\n",
    "        self.weight = Parameter(torch.ones(1, num_groups, 1))\n",
    "        self.bias = Parameter(torch.zeros(1, num_groups, 1))\n",
    "        if estimate:\n",
    "            self.bn = EstBN(num_channels)\n",
    "        else:\n",
    "            self.bn = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.bn(inp)\n",
    "        out = out.view(1, inp.size(0) * self.num_groups, -1)\n",
    "        out = torch.batch_norm(out, None, None, None, None, True, 0, self.eps, True)\n",
    "        out = out.view(inp.size(0), self.num_groups, -1)\n",
    "        out = self.weight * out + self.bias\n",
    "        out = out.view_as(inp)\n",
    "        return out\n",
    "# batch norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recreational-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading code\n",
    "class GreyscaleToRGBTransform(object):    \n",
    "    def __call__(self, image):  \n",
    "        if image.shape[0] == 1:\n",
    "            return transforms.Lambda(lambda x: x.repeat(3, 1, 1))(image)\n",
    "        return image\n",
    "    \n",
    "def get_caltech_dataset(batch_size, val_batch_size):\n",
    "    # only works with Caltech256\n",
    "    # define transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.Resize((112, 112)),\n",
    "         #transforms.Lambda(lambda x: x.repeat(3, 1, 1))  if x.shape[0] == 1  else NoneTransform(),                \n",
    "         transforms.ToTensor(),\n",
    "         GreyscaleToRGBTransform(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    val_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    # download link is broken\n",
    "    dataset = Caltech256(root=\"../data\", download=False, transform=train_transform)\n",
    "    print(dataset)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [24486, 6122])\n",
    "    train_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle = True)\n",
    "    val_set = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, shuffle = True)\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_stl_dataset(batch_size, val_batch_size):\n",
    "    # define transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.Resize((96, 96)),\n",
    "         #transforms.Lambda(lambda x: x.repeat(3, 1, 1))  if x.shape[0] == 1  else NoneTransform(),                \n",
    "         transforms.ToTensor(),\n",
    "         GreyscaleToRGBTransform(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    val_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    dataset = STL10(root=\"../data\", download=False, transform=train_transform)\n",
    "    print(dataset)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [4000, 1000])\n",
    "    train_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle = True)\n",
    "    val_set = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, shuffle = True)\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_cifar_dataset(batch_size, val_batch_size):\n",
    "    # define transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "         #transforms.Lambda(lambda x: x.repeat(3, 1, 1))  if x.shape[0] == 1  else NoneTransform(),                \n",
    "         transforms.ToTensor(),\n",
    "         #GreyscaleToRGBTransform(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    val_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    dataset = CIFAR10(root=\"../data\", download=False, transform=train_transform)\n",
    "    print(dataset)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [40000, 10000])\n",
    "    train_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers = 4, shuffle = True)\n",
    "    val_set = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, num_workers = 4, shuffle = True)\n",
    "    return train_set, val_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "central-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(num_classes):\n",
    "    models = [vgg11_bcwn(num_classes = num_classes), vgg11_gn(norm_layer = get_group_norm_layer, num_classes = num_classes), vgg11_bn(num_classes = num_classes), vgg11(num_classes = num_classes)]\n",
    "    model_names = [\"vgg11_bcwn\", \"vgg11_gn\", \"vgg_bn\", \"vgg_nn\"]\n",
    "    return models, model_names\n",
    "             \n",
    "def get_dataset_data(batch_size, val_batch_size):\n",
    "    datasets = [get_cifar_dataset(batch_size, val_batch_size), get_stl_dataset(batch_size, val_batch_size), get_caltech_dataset(batch_size, val_batch_size)]\n",
    "    dataset_names = [\"CIFAR10\", \"STL10\", \"CALTECH256\"]\n",
    "    num_classess = [10, 10, 257]\n",
    "    return datasets[2:3], dataset_names[2:3], num_classess[2:3]\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, val_loader, optimizer, clip_grad, epoch, save_name):\n",
    "    model = model.train()\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    train_corrects = []\n",
    "    train_totals = []\n",
    "    train_percents = []\n",
    "    val_losses = []\n",
    "    val_corrects = []\n",
    "    val_totals = []\n",
    "    val_percents = []\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            target = torch.as_tensor(target) # caltech256 target is int\n",
    "            data, target = data.to(device), target.to(device)        \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            #print(output.shape)\n",
    "            #print(target.shape)\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "            loss.backward()\n",
    "            if clip_grad:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            _, train_preds = torch.max(output, 1)\n",
    "            train_correct = torch.sum(train_preds == target)\n",
    "            train_total = target.shape[0]\n",
    "            train_percent = train_correct/train_total\n",
    "            train_corrects.append(train_correct)\n",
    "            train_totals.append(train_total)\n",
    "            train_percents.append(train_percent)\n",
    "            if batch_idx % args[\"log_interval\"] == 0:\n",
    "                val_loss, val_correct, val_total =  eval(model, device, val_loader, 4)\n",
    "                model = model.train()\n",
    "                optimizer.zero_grad()\n",
    "                val_losses.append(val_loss)\n",
    "                val_corrects.append(val_correct)\n",
    "                val_totals.append(val_total)\n",
    "                val_percents.append(val_correct/val_total)\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\Train_percent: {:.6f}\\t ValLoss: {:.6f}, Valpercent: {:.6f}'.format(\n",
    "                    e, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(), train_percent, val_loss.item(), val_correct/val_total))\n",
    "                if args[\"dry_run\"]:\n",
    "                    break\n",
    "        #cache model\n",
    "        path = \"{}_{}.p\".format(save_name, e)\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            \"loss\": train_losses,\n",
    "            \"val_loss\": val_losses,\n",
    "            \"train_correct\": train_corrects,\n",
    "            \"val_correct\": val_corrects,\n",
    "            \"train_total\": train_totals,\n",
    "            \"val_total\": val_totals,\n",
    "            \"train_percent\": train_percents,\n",
    "            \"val_percent\": val_percents\n",
    "            }, path)\n",
    "        print(\"saved model state and loss to {}\".format(path))\n",
    "\n",
    "def eval(model, device, val_loader, max_eval):\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        target = torch.as_tensor(target)\n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        output = model(data)\n",
    "        loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "        val_losses.append(loss.item())\n",
    "        _, val_preds = torch.max(output, 1)\n",
    "        val_correct += torch.sum(val_preds == target)\n",
    "        val_total += target.shape[0]\n",
    "        if max_eval is not None and batch_idx >= max_eval:\n",
    "            break\n",
    "    return np.mean(val_losses), val_correct, val_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weighted-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(args):\n",
    "    device = torch.device(args[\"device\"])\n",
    "    datasets, dataset_names, num_classess = get_dataset_data(args[\"batch_size\"], args[\"val_batch_size\"])\n",
    "    for num_classes, dn, (train_set, val_set) in zip(num_classess, dataset_names, datasets):\n",
    "        models, model_names = get_model_data(num_classes)\n",
    "        for model, mn in zip(models, model_names):\n",
    "            if mn == \"vgg_nn\":\n",
    "                clip_grad = True\n",
    "            else:\n",
    "                clip_grad = False\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "            name = \"{}_{}\".format(mn, dn)\n",
    "            trained_model = train(args, model, args[\"device\"], train_set, val_set, optimizer, clip_grad, args[\"epoch\"], name)\n",
    "\n",
    "def _test_parallel(args, model, mn, dn, train_set, val_set):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "    if mn == \"vgg_nn\":\n",
    "        clip_grad = True\n",
    "    else:\n",
    "        clip_grad = False\n",
    "    name = \"{}_{}\".format(mn, dn)\n",
    "    print(\"training model {} with {}\".format(mn, dn))\n",
    "    trained_model = train(args, model, args[\"device\"], train_set, val_set, optimizer, clip_grad, args[\"epoch\"], name)\n",
    "    return name\n",
    "\n",
    "def _run_test_parallel(args):\n",
    "    return _test_parallel(*args)\n",
    "\n",
    "def run_tests_parallel(args):\n",
    "    datasets, dataset_names, num_classess = get_dataset_data(args[\"batch_size\"], args[\"val_batch_size\"])\n",
    "    \n",
    "    for num_classes, dn, (train_set, val_set) in zip(num_classess, dataset_names, datasets):\n",
    "        parallel_args = []\n",
    "        models, model_names = get_model_data(num_classes)\n",
    "        pool = mp.Pool(4)\n",
    "        i = 1\n",
    "        for model, mn in zip(models, model_names):\n",
    "            args_cur = args.copy()\n",
    "            args_cur[\"device\"] = \"cuda:{}\".format(i)\n",
    "            parallel_args.append((args_cur, model, mn, dn, train_set, val_set))\n",
    "            i += 2\n",
    "        print(\"STARTING {}\".format(dn))\n",
    "        results = pool.imap_unordered(_run_test_parallel, parallel_args)\n",
    "        for x in results:\n",
    "            print(x)        \n",
    "        pool.close()\n",
    "        \n",
    "        pool.join()\n",
    "\n",
    "        print(\"Finished {}\".format(dn))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "critical-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    args = dict()\n",
    "    args[\"device\"] = \"cuda:3\" \n",
    "    args[\"lr\"] = 1e-4 # learning rate\n",
    "    args[\"epoch\"] = 10\n",
    "    args[\"batch_size\"] = 128\n",
    "    args[\"val_batch_size\"] = 128\n",
    "    args[\"log_interval\"] = 1 # 1 for stl10, 10 for other\n",
    "    args[\"dry_run\"] = False\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    # in theory load from cmd, but ... jupyter\n",
    "    args = get_args()\n",
    "    run_tests(args)\n",
    "    #run_tests_parallel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Dataset STL10\n",
      "    Number of datapoints: 5000\n",
      "    Root location: ../data\n",
      "    Split: train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(96, 96), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               <__main__.GreyscaleToRGBTransform object at 0x7f2553b86190>\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Dataset Caltech256\n",
      "    Number of datapoints: 30608\n",
      "    Root location: ../data/caltech256\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(112, 112), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               <__main__.GreyscaleToRGBTransform object at 0x7f2553b867c0>\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amrl_user/.local/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/24486 (0%)]\tLoss: 5.822037\\Train_percent: 0.000000\t ValLoss: 5.748661, Valpercent: 0.018750\n",
      "Train Epoch: 0 [128/24486 (1%)]\tLoss: 5.980876\\Train_percent: 0.031250\t ValLoss: 5.770064, Valpercent: 0.012500\n",
      "Train Epoch: 0 [256/24486 (1%)]\tLoss: 6.321559\\Train_percent: 0.031250\t ValLoss: 5.743835, Valpercent: 0.023438\n",
      "Train Epoch: 0 [384/24486 (2%)]\tLoss: 6.766642\\Train_percent: 0.031250\t ValLoss: 5.688486, Valpercent: 0.042188\n",
      "Train Epoch: 0 [512/24486 (2%)]\tLoss: 6.434398\\Train_percent: 0.007812\t ValLoss: 5.757658, Valpercent: 0.010937\n",
      "Train Epoch: 0 [640/24486 (3%)]\tLoss: 6.317764\\Train_percent: 0.015625\t ValLoss: 5.683932, Valpercent: 0.025000\n",
      "Train Epoch: 0 [768/24486 (3%)]\tLoss: 6.343917\\Train_percent: 0.007812\t ValLoss: 5.682594, Valpercent: 0.018750\n",
      "Train Epoch: 0 [896/24486 (4%)]\tLoss: 5.990362\\Train_percent: 0.007812\t ValLoss: 5.624927, Valpercent: 0.026563\n",
      "Train Epoch: 0 [1024/24486 (4%)]\tLoss: 5.972514\\Train_percent: 0.015625\t ValLoss: 5.627977, Valpercent: 0.025000\n",
      "Train Epoch: 0 [1152/24486 (5%)]\tLoss: 6.139318\\Train_percent: 0.015625\t ValLoss: 5.621233, Valpercent: 0.026563\n",
      "Train Epoch: 0 [1280/24486 (5%)]\tLoss: 6.006443\\Train_percent: 0.007812\t ValLoss: 5.577593, Valpercent: 0.053125\n",
      "Train Epoch: 0 [1408/24486 (6%)]\tLoss: 6.086442\\Train_percent: 0.015625\t ValLoss: 5.538904, Valpercent: 0.025000\n",
      "Train Epoch: 0 [1536/24486 (6%)]\tLoss: 6.034771\\Train_percent: 0.007812\t ValLoss: 5.533980, Valpercent: 0.028125\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code\n",
    "def plot_losses(dn, mnlosses, title):\n",
    "    plt.figure()\n",
    "    for mn, loss in mnlosses:\n",
    "        plt.plot(loss, label=mn)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"Cross Entropy Loss\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(\"../figure/{}.png\".format(title))\n",
    "\n",
    "def plot_accuracy(dn, mnpercents, title):\n",
    "    plt.figure()\n",
    "    for mn, accuracy in mnpercents:\n",
    "        plt.plot(accuracy, label=mn)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"percent\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(\"../figure/{}.png\".format(title))\n",
    "    \n",
    "def plot_all(args, MAX_EPOCH):\n",
    "    device = torch.device(args[\"device\"])\n",
    "    datasets, dataset_names, num_classess = get_dataset_data(args[\"batch_size\"], args[\"val_batch_size\"])\n",
    "    cache_data = dict()\n",
    "    for num_classes, dn, (train_set, val_set) in zip(num_classess, dataset_names, datasets):\n",
    "        models, model_names = get_model_data(num_classes)\n",
    "        for model, mn in zip(models, model_names):\n",
    "            if mn == \"vgg_nn\":\n",
    "                clip_grad = True\n",
    "            else:\n",
    "                clip_grad = False\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "            checkpt_name = \"{}_{}_{}.p\".format(mn, dn, MAX_EPOCH)\n",
    "            checkpoint = torch.load(checkpt_name)\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            cache_data[(dn, mn)] = checkpoint\n",
    "            #plot_train_loss(mn, dn, train_losses)\n",
    "            #return\n",
    "    # plot the training loss\n",
    "    for dn in dataset_names:\n",
    "        mntlosses = [(mn, cache_data[(dn, mn)][\"loss\"]) for mn in model_names]\n",
    "        mnvlosses = [(mn, cache_data[(dn, mn)][\"val_loss\"]) for mn in model_names]\n",
    "        mntaccuracy = [(mn, cache_data[(dn, mn)][\"train_percent\"]) for mn in model_names]\n",
    "        mnvaccuracy = [(mn, cache_data[(dn, mn)][\"val_percent\"]) for mn in model_names]\n",
    "        \n",
    "        plot_losses(dn, mntlosses, \"Training loss across all models for {}\".format(dn))\n",
    "        plot_losses(dn, mnvlosses, \"Validation loss across all models for {}\".format(dn))\n",
    "        plot_accuracy(dn, mntaccuracy, \"Training accuracy across all models for {}\".format(dn))\n",
    "        plot_accuracy(dn, mnvaccuracy, \"Validation accuracy across all models for {}\".format(dn))\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "plot_all(get_args(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-stanford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-genesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
