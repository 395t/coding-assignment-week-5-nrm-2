{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5aba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torchvision\n",
    "from typing import Union, List, Dict, Any, cast\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import Caltech256, CIFAR10, STL10, Caltech101\n",
    "from torchvision.models import VGG\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a07096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group norm\n",
    "def make_gn_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                if norm_layer is None:\n",
    "                    raise Error(\"Please specify a norm layer\")\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                layers += [conv2d, norm_layer(v//2, v)(), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_gn_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_gn_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11_gn(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_gn_vgg('vgg11_bn', 'A', True, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3bcfafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel batch\n",
    "def make_bc_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = WConv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                # hyperparameters\n",
    "                num_groups = v//2\n",
    "                eps=1e-08\n",
    "                layers += [conv2d, BCNorm(v, num_groups, eps), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_bc_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_bc_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11_bcwn(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_bc_vgg('vgg11_bn', 'A', True, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12d1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel batch\n",
    "def make_bn_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                # hyperparameters\n",
    "                eps=1e-08\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_bn_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_bn_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11_bn(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_bn_vgg('vgg11_bn', 'A', True, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9be4ddd0",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b70224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel batch\n",
    "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False, norm_layer = None) -> nn.Sequential:\n",
    "    layers: List[nn.Module] = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            v = cast(int, v)\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            # v is the output channel\n",
    "            if batch_norm:\n",
    "                # @group if want to use this, please refer to the higher order function\n",
    "                # in the next block\n",
    "                # hyperparameters\n",
    "                raise NotImplemented\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, norm_layer=None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    cfgs: Dict[str, List[Union[str, int]]] = {\n",
    "        'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm, norm_layer=norm_layer), num_classes = num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        raise NotImplementedError()\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def vgg11(pretrained: bool = False, progress: bool = True, norm_layer = None, num_classes = None, **kwargs: Any) -> VGG:\n",
    "    r\"\"\"\n",
    "    Makes the group norm version of VGG11\n",
    "    VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\n",
    "    The required minimum input size of the model is 32x32.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    assert num_classes is not None, \"give a number of class in accordance to dataset\"\n",
    "    return make_vgg('vgg11', 'A', False, pretrained, progress, norm_layer = norm_layer, num_classes = num_classes, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1f7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_group_norm_layer(in_channel, out_channel):\n",
    "    def fun():\n",
    "        return nn.GroupNorm(in_channel, out_channel)\n",
    "    return fun\n",
    "\n",
    "# channel norm + weight\n",
    "# weight\n",
    "class WConv2d(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(WConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                 padding, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.weight\n",
    "        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n",
    "                                  keepdim=True).mean(dim=3, keepdim=True)\n",
    "        weight = weight - weight_mean\n",
    "        std = torch.pow(weight.view(weight.size(0), -1).var(dim=1) + 1e-5, 0.5).view(-1, 1, 1, 1)\n",
    "        weight = weight / std.expand_as(weight)\n",
    "        return F.conv2d(x, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "\n",
    "class BCNorm(nn.Module):\n",
    "#     eps = epsilon\n",
    "    def __init__(self, num_channels, num_groups, eps, estimate=False):\n",
    "        super(BCNorm, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_groups = num_groups\n",
    "        self.eps = eps\n",
    "        self.weight = Parameter(torch.ones(1, num_groups, 1))\n",
    "        self.bias = Parameter(torch.zeros(1, num_groups, 1))\n",
    "        if estimate:\n",
    "            self.bn = EstBN(num_channels)\n",
    "        else:\n",
    "            self.bn = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out = self.bn(inp)\n",
    "        out = out.view(1, inp.size(0) * self.num_groups, -1)\n",
    "        out = torch.batch_norm(out, None, None, None, None, True, 0, self.eps, True)\n",
    "        out = out.view(inp.size(0), self.num_groups, -1)\n",
    "        out = self.weight * out + self.bias\n",
    "        out = out.view_as(inp)\n",
    "        return out\n",
    "# batch norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recreational-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading code\n",
    "class GreyscaleToRGBTransform(object):    \n",
    "    def __call__(self, image):  \n",
    "        if image.shape[0] == 1:\n",
    "            return transforms.Lambda(lambda x: x.repeat(3, 1, 1))(image)\n",
    "        return image\n",
    "    \n",
    "def get_caltech_dataset(batch_size):\n",
    "    # only works with Caltech256\n",
    "    # define transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         #transforms.Lambda(lambda x: x.repeat(3, 1, 1))  if x.shape[0] == 1  else NoneTransform(),                \n",
    "         transforms.ToTensor(),\n",
    "         GreyscaleToRGBTransform(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    val_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    # download link is broken\n",
    "    dataset = Caltech256(root=\"../data\", download=False, transform=train_transform)\n",
    "    print(dataset)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [24486, 6122])\n",
    "    train_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    val_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_stl_dataset(batch_size):\n",
    "    # define transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         #transforms.Lambda(lambda x: x.repeat(3, 1, 1))  if x.shape[0] == 1  else NoneTransform(),                \n",
    "         transforms.ToTensor(),\n",
    "         GreyscaleToRGBTransform(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    val_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    dataset = STL10(root=\"../data\", download=False, transform=train_transform)\n",
    "    print(dataset)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [4000, 1000])\n",
    "    train_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    val_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    return train_set, val_set\n",
    "\n",
    "def get_cifar_dataset(batch_size):\n",
    "    # define transforms\n",
    "    train_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         #transforms.Lambda(lambda x: x.repeat(3, 1, 1))  if x.shape[0] == 1  else NoneTransform(),                \n",
    "         transforms.ToTensor(),\n",
    "         GreyscaleToRGBTransform(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    val_transform = transforms.Compose(\n",
    "        [transforms.Resize((224, 224)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])])\n",
    "    dataset = CIFAR10(root=\"../data\", download=True, transform=train_transform)\n",
    "    print(dataset)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [40000, 10000])\n",
    "    train_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    val_set = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    return train_set, val_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "central-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, clip_grad, epoch, save_name):\n",
    "    model = model.train()\n",
    "    model = model.to(device)\n",
    "    train_losses = []\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            target = torch.as_tensor(target) # caltech256 target is int\n",
    "            data, target = data.to(device), target.to(device)        \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            #print(output.shape)\n",
    "            #print(target.shape)\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "            loss.backward()\n",
    "            if clip_grad:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            if batch_idx % args[\"log_interval\"] == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    e, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "                if args[\"dry_run\"]:\n",
    "                    break\n",
    "        #cache model\n",
    "        path = \"{}_{}.p\".format(save_name, e)\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            \"loss\": train_losses\n",
    "            }, path)\n",
    "        print(\"saved model state and loss to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weighted-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(args):\n",
    "    datasets = [get_cifar_dataset(args[\"batch_size\"]), get_stl_dataset(args[\"batch_size\"]), get_caltech_dataset(args[\"batch_size\"])]\n",
    "    dataset_names = [\"CIFAR10\", \"STL10\", \"CALTECH256\"]\n",
    "    device = torch.device(args[\"device\"])\n",
    "    num_classess = [10, 10, 257]\n",
    "    for num_classes, dn, (train_set, val_set) in zip(num_classess, dataset_names, datasets):\n",
    "        models = [vgg11_bcwn(num_classes = num_classes), vgg11_gn(norm_layer = get_group_norm_layer, num_classes = num_classes), vgg11_bn(num_classes = num_classes), vgg11(num_classes = num_classes)]\n",
    "        model_names = [\"vgg11_bcwn\", \"vgg11_gn\", \"vgg_bn\", \"vgg_nn\"]\n",
    "        for model, mn in zip(models, model_names):\n",
    "            if mn == \"vgg_nn\":\n",
    "                clip_grad = True\n",
    "            else:\n",
    "                clip_grad = False\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "            name = \"{}_{}\".format(mn, dn)\n",
    "            trained_model = train(args, model, args[\"device\"], train_set, optimizer, clip_grad, args[\"epoch\"], name)\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "critical-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # in theory load from cmd, but ... jupyter\n",
    "    args = dict()\n",
    "    args[\"device\"] = \"cuda:3\" \n",
    "    args[\"lr\"] = 1e-4 # learning rate\n",
    "    args[\"epoch\"] = 1\n",
    "    args[\"batch_size\"] = 32\n",
    "    args[\"log_interval\"] = 10\n",
    "    args[\"dry_run\"] = True\n",
    "    run_tests(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               <__main__.GreyscaleToRGBTransform object at 0x7f91002e79d0>\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Dataset STL10\n",
      "    Number of datapoints: 5000\n",
      "    Root location: ../data\n",
      "    Split: train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               <__main__.GreyscaleToRGBTransform object at 0x7f907e265730>\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Dataset Caltech256\n",
      "    Number of datapoints: 30608\n",
      "    Root location: ../data/caltech256\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               <__main__.GreyscaleToRGBTransform object at 0x7f907e265880>\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amrl_user/.local/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/40000 (0%)]\tLoss: 2.576375\n",
      "saved model state and loss to vgg11_bcwn_CIFAR10_0.p\n",
      "Train Epoch: 0 [0/40000 (0%)]\tLoss: 2.810745\n",
      "saved model state and loss to vgg11_gn_CIFAR10_0.p\n",
      "Train Epoch: 0 [0/40000 (0%)]\tLoss: 2.518934\n",
      "saved model state and loss to vgg_bn_CIFAR10_0.p\n",
      "Train Epoch: 0 [0/40000 (0%)]\tLoss: 2.296564\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-stanford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-genesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
